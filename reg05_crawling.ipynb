{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Web Crawling**\n",
    "## **01 웹페이지 정보 확인**\n",
    "```bash\n",
    "$ pip install python-whois\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'domain_name': 'aladin.co.kr',\n",
       " 'registrant_name': 'Aladin Communications Inc.',\n",
       " 'registrant_address': None,\n",
       " 'registrant_zip': None,\n",
       " 'admin_name': 'Aladdin communications Inc',\n",
       " 'admin_email': 'webmaster@aladin.co.kr',\n",
       " 'admin_phone': None,\n",
       " 'creation_date': datetime.datetime(1999, 1, 15, 0, 0),\n",
       " 'updated_date': datetime.datetime(2019, 1, 10, 0, 0),\n",
       " 'expiration_date': datetime.datetime(2023, 10, 15, 0, 0),\n",
       " 'registrar': 'Inames Co., Ltd.(http://www.inames.co.kr)',\n",
       " 'name_servers': ['dns.aladdin.co.kr',\n",
       "  'ns.intellicenter.co.kr',\n",
       "  'ns3.intellicenter.co.kr']}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import whois\n",
    "whois.whois('http://used.aladin.co.kr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **02 다운로드 재시도**\n",
    "- 500 오류가 발생시 재시도를 한다\n",
    "- `hasattr()` : 객체에 속성값을 갖고 있는지 판정 (`boolean`) 하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading:  http://httpstat.us/500\n",
      "Download error:  Internal Server Error\n",
      "Downloading:  http://httpstat.us/500\n",
      "Download error:  Internal Server Error\n",
      "Downloading:  http://httpstat.us/500\n",
      "Download error:  Internal Server Error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error   import HTTPError\n",
    "\n",
    "def download(url, number_retries = 2):\n",
    "    print('Downloading: ', url)\n",
    "    try:\n",
    "        html = urlopen(url).read().decode('utf-8')\n",
    "    except HTTPError as e:\n",
    "        print('Download error: ', e.reason)\n",
    "        html = None\n",
    "        if number_retries > 0 :\n",
    "            # 5xx HTML 오류시 재시도\n",
    "            if hasattr(e, 'code') and (500 <= e.code <= 600):\n",
    "                return download(url, number_retries - 1)\n",
    "    return html\n",
    "\n",
    "response = download('http://httpstat.us/500')\n",
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading:  http://www.samsung.com\n",
      "Download error:  Forbidden\n"
     ]
    }
   ],
   "source": [
    "# 파이선 크롤링 자체를 막는 사이트\n",
    "download('http://www.samsung.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **03 웹링크자료 다운로드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading:  http://example.webscraping.com/sitemap.xml\n",
      "Download error:  Not Found\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from urllib.request import Request\n",
    "\n",
    "def crawl_sitemap(url):\n",
    "    sitemap = download(url)                          # sitemap 파일 다운로드\n",
    "    if sitemap is not None:\n",
    "        links = re.findall('<loc>(.*?)</loc>', sitemap)  # sitemap 링크 추출 \n",
    "        for link in links:                               # 추출된 각 링크 다운로드\n",
    "            html = download(link)                        # 스크랩된 html\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "crawl_sitemap('http://example.webscraping.com/sitemap.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **04 프록시 지원 접근**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import Request\n",
    "import urllib.robotparser\n",
    "\n",
    "def download(url, user_agent='wswp', proxy=None, num_retries=2):\n",
    "    print('Downloding:', url)\n",
    "    headers = {'User-agent': user_agent}\n",
    "    request = Request(url, headers=headers)\n",
    "    opener = urllib.request.build_opener()\n",
    "    if proxy:\n",
    "        proxy_params = {urllib.request.urlparse(url).scheme: proxy}\n",
    "        opener.add_handler(urllib.request.ProxyHandler(proxy_params))\n",
    "    try:\n",
    "        html = urlopen(request).read().decode('utf-8')\n",
    "    except HTTPError as e:\n",
    "        print('Download error:', e.reason)\n",
    "        html = None\n",
    "        if num_retries > 0:\n",
    "            if hasattr(e, 'code') and 500 <= e.code < 600:\n",
    "                # 5xx HTTP 오류시 재시도\n",
    "                return download(url, num_retries - 1)\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_crawler(seed_url, link_regex):\n",
    "    crawl_queue = [seed_url]      # 이전에 다운로드한 URL 저장\n",
    "    seen = set(crawl_queue)\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url('http://example.webscraping.com/robots.txt')\n",
    "    rp.read()\n",
    "    user_agent = 'GoodCrawler'     # 이 아래 user_agent를 번갈아 보면서 실행하면 결과를 알 수 있다\n",
    "    # user_agent = 'BadCrawler'\n",
    "    while crawl_queue:\n",
    "        url = crawl_queue.pop()\n",
    "        if rp.can_fetch(user_agent, url):\n",
    "            html = download(url)\n",
    "            for link in get_links(html):\n",
    "                if re.match(link_regex, link):   # 링크가 regexd와 일치하는지 확인\n",
    "                    link = urllib.parse.urljoin(seed_url, link) # 절대 링크 생성\n",
    "                    if link not in seen:  # 현재 링크가 이전에 다운로드한 링크인지 확인\n",
    "                        seen.add(link)\n",
    "                        crawl_queue.append(link)\n",
    "        else:\n",
    "            print('Blocked by robots.txt:', seed_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloding: http://example.webscraping.com\n"
     ]
    }
   ],
   "source": [
    "def get_links(html):\n",
    "    webpage_regex = re.compile('<a[^>]+href=[\"\\'](.*?)[\"\\']',re.IGNORECASE)  # 웹페이지에서 모든 링크를 추출하는 정규식\n",
    "    return webpage_regex.findall(html)                                       # 웹페이지의 모든 링크 목록\n",
    "\n",
    "link_crawler('http://example.webscraping.com', '/(index|view)/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **05 다운로드 조절하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link_crawler_delay.py\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "class Throttle:\n",
    "    def __init__(self, delay):\n",
    "        self.delay = delay # 각 도메인의 다운로드 사이에 지연 시간\n",
    "        self.domains = {}  # 도메인을 마지막으로 접속한 시간\n",
    "    def wait(self, url):\n",
    "        domain = urllib.parse.urlparse(url).netloc\n",
    "        last_accessed = self.domains.get(domain)\n",
    "        if self.delay > 0 and last_accessed is not None:\n",
    "            sleep_secs = self.delay - (datetime.datetime.now() - last_accessed).seconds\n",
    "            if sleep_secs > 0:          # 도메인을 최근에 접속하였으며, 지연을 할 필요가 있다\n",
    "                time.sleep(sleep_secs)  # 최근에 접속한 시간 최신화\n",
    "        self.domains[domain] = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, user_agent='wswp', proxy=None, num_retries=2):\n",
    "    print('Downloding:', url)\n",
    "    headers = {'User-agent': user_agent}\n",
    "    request = Request(url, headers=headers)\n",
    "    opener = urllib.request.build_opener()\n",
    "    if proxy:\n",
    "        proxy_params = {urllib.request.urlparse(url).scheme: proxy}\n",
    "        opener.add_handler(urllib.request.ProxyHandler(proxy_params))\n",
    "    try:\n",
    "        html = urlopen(request).read().decode('utf-8')\n",
    "    except HTTPError as e:\n",
    "        print('Download error:', e.reason)\n",
    "        html = None\n",
    "        if num_retries > 0:\n",
    "            if hasattr(e, 'code') and 500 <= e.code < 600:\n",
    "                return download(url, num_retries - 1)   # 5xx HTTP 오류시 재시도\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_crawler(seed_url, link_regex):\n",
    "    crawl_queue = [seed_url]\n",
    "    seen = set(crawl_queue)     # 이전에 다운로드한 URL 저장\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url('http://example.webscraping.com/robots.txt') ; rp.read()\n",
    "    user_agent = 'GoodCrawler'     # 이 아래 user_agent를 번갈아 보면서 실행하면 결과를 알 수 있다\n",
    "    # user_agent = 'BadCrawler'\n",
    "    delay = 2 ; throttle = Throttle(delay)\n",
    "    while crawl_queue:\n",
    "        url = crawl_queue.pop()\n",
    "        if rp.can_fetch(user_agent, url):\n",
    "            throttle.wait(url) ; html = download(url)\n",
    "            for link in get_links(html):\n",
    "                if re.match(link_regex, link):                  # 링크가 regexd와 일치하는지 확인\n",
    "                    link = urllib.parse.urljoin(seed_url, link) # 절대 링크 생성\n",
    "                    if link not in seen:                        # 현재 링크가 이전에 다운로드한 링크인지 확인\n",
    "                        seen.add(link) ; crawl_queue.append(link)\n",
    "        else:\n",
    "            print('Blocked by robots.txt:', seed_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloding: http://example.webscraping.com\n"
     ]
    }
   ],
   "source": [
    "def get_links(html):\n",
    "    webpage_regex = re.compile('<a[^>]+href=[\"\\'](.*?)[\"\\']',re.IGNORECASE)\n",
    "    return webpage_regex.findall(html)\n",
    "\n",
    "link_crawler('http://example.webscraping.com', '/(index|view)/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **06 Download**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hotel_name = tree.xpath(xpath)\n",
    "# print(len(tree.xpath(xpath)))\n",
    "# temp = re.sub(\"\\n\",\"\",hotel_name[0])\n",
    "# temp = re.sub(\"  \",\"\",temp); temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests, re\n",
    "url = 'https://developer.mozilla.org/en-US/docs/Tools/Settings'\n",
    "page = requests.get(url)\n",
    "tree = html.fromstring(page.content) ; #page.text[:300]\n",
    "node = '//*[@id=\"wikiArticle\"]/ul/li[1]/node()'\n",
    "for txt in tree.xpath(node):\n",
    "    print('Node :', txt)\n",
    "text = '//*[@id=\"wikiArticle\"]/ul/li[1]/text()'\n",
    "print('----------------------------------------------------')\n",
    "for txt in tree.xpath(text):\n",
    "    print('Text :', txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiThread Download\n",
    "- https://bitbucket.org/wswp/code/src/9e6b82b47087c2ada0e9fdf4f5e037e151975f0f/chapter04/threaded_test.py?at=default&fileviewer=file-view-default\n",
    "- https://stackoverflow.com/questions/32285453/why-does-multithreading-do-not-speed-up-parsing-html-with-lxmlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml.html import fromstring, HTMLParser\n",
    "import time\n",
    "from threading import Thread\n",
    "from urllib.request import urlopen\n",
    "def func(number):\n",
    "    parser = HTMLParser()\n",
    "    for x in range(number):\n",
    "        fromstring(DATA, parser=parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = urlopen('http://lxml.de/FAQ.html').read()\n",
    "print('Testing one thread (100 job per thread)')\n",
    "start = time.time()\n",
    "t1 = Thread(target=func, args=[100])\n",
    "t1.start();    t1.join();    elapsed = time.time() - start\n",
    "print('Time: %.5f' % elapsed)\n",
    "print('Testing two threads (50 jobs per thread)')\n",
    "start = time.time() \n",
    "t1 = Thread(target=func, args=[50])\n",
    "t2 = Thread(target=func, args=[50])\n",
    "t1.start();   t2.start();   t1.join();  t2.join();  elapsed = time.time() - start\n",
    "print('Time: %.5f' % elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bs4 01\n",
    "#url = 'http://example.webscraping.com/view/United Kingdom-239'\n",
    "#html = download(url)\n",
    "# soup.find( text = re.compile(\"sisters\") )              # text 내용이 sisters 를 포함시\n",
    "# soup.find( attrs = {'id' : 'place_area_row'} )\n",
    "# soup.find( attrs = {'class' : 'w2p_fw'} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bs4 02 \n",
    "# soup.find_all( \"p\")           # <tag>\n",
    "# soup.find_all( \"p\", \"title\" )  # CSS클래스 title 인 <p> tag의 값을 추출\n",
    "# soup.find_all( id = \"link2\" )    # id\n",
    "# soup.find_all( id = True)                              # id 속성을 포함시\n",
    "# soup.find_all( href = re.compile(\"elsie\") )            # elsie URL 링크 포함시\n",
    "# soup.find_all( href = re.compile(\"elsie\"), id='link1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Expression 01\n",
    "# print(re.findall('<td class=\"w2p_fw\">(.*?)</td>', html)[0])\n",
    "# print(re.findall('<tr id=\"places_area__row\">.*?<td\\s*class=[\"\\']w2p_fw[\"\\']>(.*?)</td>', html))\n",
    "# print(re.findall('<tr id=\"places_area__row\"><td class=\"w2p_fl\"><label for=\"places_area\" id=\"places_area__label\">\\\n",
    "#                   Area: </label></td><td class=\"w2p_fw\">(.*?)</td>', html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Expression 02\n",
    "# url = 'http://example.webscraping.com/view/United Kingdom-239'\n",
    "# html = download(url)\n",
    "# re.findall(\"<td class='w2p_fw'>(.*?)</td>\", html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Expression (제일 빠름)\n",
    "# results[field] = re.search('<tr id=\"places_%s__row\">.*?<td class=\"w2p_fw\">(.*?)</td>' % field, html).groups()[0]\n",
    "# lxml  (중간)\n",
    "# tree = lxml.html.fromstring(html)\n",
    "# results[field] = tree.cssselect('table > tr#places_%s__row> td.w2p_fw' % field)[0].text_content()\n",
    "# BS4   (제일 느리다)\n",
    "# soup = BeautifulSoup(html, 'html.parser')\n",
    "# results[field] = soup.find('table').find('tr',id='places_%s__row' % field).find('td', class_='w2p_fw').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LXML\n",
    "- C로 만든 XML 분석용 라이브러리\n",
    "- 설치는 어렵지만 가장 강력한 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bs4로 활용찾고, Re로 완성하기\n",
    "# Regular Expression (용이성은 떨어지지만, 제일 빠르고 파이썬 내부 모듈을 활용) - bs4로 활용찾고, Re로 완성하기\n",
    "# results[field] = re.search('<tr id=\"places_%s__row\">.*?<td class=\"w2p_fw\">(.*?)</td>' % field, html).groups()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_population_lxml.py\n",
    "import re\n",
    "from lxml import html\n",
    "broken_html = '<ul class=country><li>Area<li>Population</ul>'\n",
    "tree = html.fromstring(broken_html)  # HTML 분석\n",
    "fixed_html = html.tostring(tree, pretty_print=True); fixed_html.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "url = 'http://media.daum.net/ranking/empathy'\n",
    "page = requests.get(url)\n",
    "tree = html.fromstring(page.content)\n",
    "page.text[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = '//*[@id=\"mArticle\"]/div[2]/ul[2]/li[1]/div[2]/div/span/text()'\n",
    "hotel_name = tree.xpath(xpath)\n",
    "temp = re.sub(\"\\n\",\"\",hotel_name[0])\n",
    "temp = re.sub(\"  \",\"\",temp); temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .list_news2 > li:nth-child(1) > div:nth-child(3) > strong:nth-child(1) > a:nth-child(1)\n",
    "# //*[@id=\"mArticle\"]/div[2]/ul[2]/li[1]/div[2]/strong/a\n",
    "# //*[@id=\"mArticle\"]/div[2]/ul[2]/li[2]/div[2]/strong/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = '//*[@id=\"mArticle\"]/div[2]/ul[2]/li['+str(n)+']/div[2]/strong/a/text()'\n",
    "hotel_name = tree.xpath(xpath)\n",
    "temp = re.sub(\"\\n\",\"\",hotel_name[0])\n",
    "temp = re.sub(\"  \",\"\",temp); temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = '//*[@id=\"mArticle\"]/div[2]/ul[2]/li['+str(n)+']/div[2]/div/span/text()'\n",
    "hotel_name = tree.xpath(xpath)\n",
    "temp = re.sub(\"\\n\",\"\",hotel_name[0]) \n",
    "temp = re.sub(\"  \",\"\",temp); temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Crawling to CSV DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from io import StringIO\n",
    "from csv import DictWriter\n",
    "f= StringIO('''\n",
    "    <html><body>\n",
    "    <a class=\"ui-magnifier-glass\" \n",
    "       href=\"here goes the link that i want to extract\" data-spm-anchor-id=\"0.0.0.0\" \n",
    "       style=\"width: 258px; height: 258px; position: absolute; left: -1px; top: -1px; display: none;\"></a>\n",
    "    <a href=\"link to extract\" title=\"title to extract\" rel=\"category tag\" data-spm-anchor-id=\"0.0.0.0\">\n",
    "    or maybe this word instead of title</a>\n",
    "    </body></html>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = etree.parse(f); data=[]\n",
    "r = doc.xpath('//a[@data-spm-anchor-id=\"0.0.0.0\"]')\n",
    "for elem in r:\n",
    "    link=elem.get('href')\n",
    "    title=elem.get('title')\n",
    "    text=elem.text\n",
    "    data.append({'link': link,'title': title,'text': text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('file.csv', 'w') as csvfile:\n",
    "    fieldnames=['link', 'title', 'text']\n",
    "    writer = DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in data:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('./file.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
