{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Web Crawling**\n",
    "## **01 웹페이지 정보 확인**\n",
    "```bash\n",
    "$ pip install python-whois\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bs4 01\n",
    "#url = 'http://example.webscraping.com/view/United Kingdom-239'\n",
    "#html = download(url)\n",
    "# soup.find( text = re.compile(\"sisters\") )              # text 내용이 sisters 를 포함시\n",
    "# soup.find( attrs = {'id' : 'place_area_row'} )\n",
    "# soup.find( attrs = {'class' : 'w2p_fw'} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bs4 02 \n",
    "# soup.find_all( \"p\")           # <tag>\n",
    "# soup.find_all( \"p\", \"title\" )  # CSS클래스 title 인 <p> tag의 값을 추출\n",
    "# soup.find_all( id = \"link2\" )    # id\n",
    "# soup.find_all( id = True)                              # id 속성을 포함시\n",
    "# soup.find_all( href = re.compile(\"elsie\") )            # elsie URL 링크 포함시\n",
    "# soup.find_all( href = re.compile(\"elsie\"), id='link1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Expression 01\n",
    "# print(re.findall('<td class=\"w2p_fw\">(.*?)</td>', html)[0])\n",
    "# print(re.findall('<tr id=\"places_area__row\">.*?<td\\s*class=[\"\\']w2p_fw[\"\\']>(.*?)</td>', html))\n",
    "# print(re.findall('<tr id=\"places_area__row\"><td class=\"w2p_fl\"><label for=\"places_area\" id=\"places_area__label\">\\\n",
    "#                   Area: </label></td><td class=\"w2p_fw\">(.*?)</td>', html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Expression 02\n",
    "# url = 'http://example.webscraping.com/view/United Kingdom-239'\n",
    "# html = download(url)\n",
    "# re.findall(\"<td class='w2p_fw'>(.*?)</td>\", html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Expression (제일 빠름)\n",
    "# results[field] = re.search('<tr id=\"places_%s__row\">.*?<td class=\"w2p_fw\">(.*?)</td>' % field, html).groups()[0]\n",
    "# lxml  (중간)\n",
    "# tree = lxml.html.fromstring(html)\n",
    "# results[field] = tree.cssselect('table > tr#places_%s__row> td.w2p_fw' % field)[0].text_content()\n",
    "# BS4   (제일 느리다)\n",
    "# soup = BeautifulSoup(html, 'html.parser')\n",
    "# results[field] = soup.find('table').find('tr',id='places_%s__row' % field).find('td', class_='w2p_fw').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LXML\n",
    "- C로 만든 XML 분석용 라이브러리\n",
    "- 설치는 어렵지만 가장 강력한 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bs4로 활용찾고, Re로 완성하기\n",
    "# Regular Expression (용이성은 떨어지지만, 제일 빠르고 파이썬 내부 모듈을 활용) - bs4로 활용찾고, Re로 완성하기\n",
    "# results[field] = re.search('<tr id=\"places_%s__row\">.*?<td class=\"w2p_fw\">(.*?)</td>' % field, html).groups()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_population_lxml.py\n",
    "import re\n",
    "from lxml import html\n",
    "broken_html = '<ul class=country><li>Area<li>Population</ul>'\n",
    "tree = html.fromstring(broken_html)  # HTML 분석\n",
    "fixed_html = html.tostring(tree, pretty_print=True); fixed_html.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "url = 'http://media.daum.net/ranking/empathy'\n",
    "page = requests.get(url)\n",
    "tree = html.fromstring(page.content)\n",
    "page.text[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = '//*[@id=\"mArticle\"]/div[2]/ul[2]/li[1]/div[2]/div/span/text()'\n",
    "hotel_name = tree.xpath(xpath)\n",
    "temp = re.sub(\"\\n\",\"\",hotel_name[0])\n",
    "temp = re.sub(\"  \",\"\",temp); temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .list_news2 > li:nth-child(1) > div:nth-child(3) > strong:nth-child(1) > a:nth-child(1)\n",
    "# //*[@id=\"mArticle\"]/div[2]/ul[2]/li[1]/div[2]/strong/a\n",
    "# //*[@id=\"mArticle\"]/div[2]/ul[2]/li[2]/div[2]/strong/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = '//*[@id=\"mArticle\"]/div[2]/ul[2]/li['+str(n)+']/div[2]/strong/a/text()'\n",
    "hotel_name = tree.xpath(xpath)\n",
    "temp = re.sub(\"\\n\",\"\",hotel_name[0])\n",
    "temp = re.sub(\"  \",\"\",temp); temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = '//*[@id=\"mArticle\"]/div[2]/ul[2]/li['+str(n)+']/div[2]/div/span/text()'\n",
    "hotel_name = tree.xpath(xpath)\n",
    "temp = re.sub(\"\\n\",\"\",hotel_name[0]) \n",
    "temp = re.sub(\"  \",\"\",temp); temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Crawling to CSV DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from io import StringIO\n",
    "from csv import DictWriter\n",
    "f= StringIO('''\n",
    "    <html><body>\n",
    "    <a class=\"ui-magnifier-glass\" \n",
    "       href=\"here goes the link that i want to extract\" data-spm-anchor-id=\"0.0.0.0\" \n",
    "       style=\"width: 258px; height: 258px; position: absolute; left: -1px; top: -1px; display: none;\"></a>\n",
    "    <a href=\"link to extract\" title=\"title to extract\" rel=\"category tag\" data-spm-anchor-id=\"0.0.0.0\">\n",
    "    or maybe this word instead of title</a>\n",
    "    </body></html>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = etree.parse(f); data=[]\n",
    "r = doc.xpath('//a[@data-spm-anchor-id=\"0.0.0.0\"]')\n",
    "for elem in r:\n",
    "    link=elem.get('href')\n",
    "    title=elem.get('title')\n",
    "    text=elem.text\n",
    "    data.append({'link': link,'title': title,'text': text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('file.csv', 'w') as csvfile:\n",
    "    fieldnames=['link', 'title', 'text']\n",
    "    writer = DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in data:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('./file.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
