{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch turorial 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST 1\n",
    "softmax를 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa047a330c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, random\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from time import time\n",
    "torch.manual_seed(777)  # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "t0 = time()\n",
    "# MNIST dataset\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "# dataset loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model = torch.nn.Linear(784, 10, bias=True)\n",
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1] cost = 0.615321696\n",
      "[Epoch:    2] cost = 0.343970388\n",
      "[Epoch:    3] cost = 0.308309406\n",
      "[Epoch:    4] cost = 0.291389644\n",
      "[Epoch:    5] cost = 0.281003147\n",
      "[Epoch:    6] cost = 0.274250835\n",
      "[Epoch:    7] cost = 0.268962383\n",
      "[Epoch:    8] cost = 0.264928371\n",
      "[Epoch:    9] cost = 0.261347979\n",
      "[Epoch:   10] cost = 0.258764029\n",
      "[Epoch:   11] cost = 0.25649184\n",
      "[Epoch:   12] cost = 0.254417121\n",
      "[Epoch:   13] cost = 0.252568364\n",
      "[Epoch:   14] cost = 0.251057833\n",
      "[Epoch:   15] cost = 0.24962458\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "# 학습모델 만들기\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(mnist_train) // batch_size\n",
    "    \n",
    "    for i, (batch_xs, batch_ys) in enumerate(data_loader):\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        X = Variable(batch_xs.view(-1, 28 * 28))\n",
    "        Y = Variable(batch_ys)    # label 은 one-hot encoded 이 아니다 (convolution 알고리즘)\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost / total_batch\n",
    "    print(\"[Epoch: {:>4}] cost = {:>.9}\".format(epoch + 1, avg_cost.data[0]))\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9039\n"
     ]
    }
   ],
   "source": [
    "# 학습모델 테스트 및 정확도 측정\n",
    "X_test = Variable(mnist_test.test_data.view(-1, 28 * 28).float())\n",
    "Y_test = Variable(mnist_test.test_labels)\n",
    "\n",
    "prediction = model(X_test)\n",
    "correct_prediction = (torch.max(prediction.data, 1)[1] == Y_test.data)\n",
    "accuracy = correct_prediction.float().mean()\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  \n",
      " 3\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "Prediction:  \n",
      " 3\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "2 min 15 sec\n"
     ]
    }
   ],
   "source": [
    "# 임의 데이터 시뮬레이션\n",
    "r = random.randint(0, len(mnist_test) - 1)\n",
    "\n",
    "# 학습모형으로 시뮬레이션\n",
    "X_single_data = Variable(mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float())\n",
    "Y_single_data = Variable(mnist_test.test_labels[r:r + 1])\n",
    "print(\"Label: \", Y_single_data.data)\n",
    "\n",
    "# 실제 데이터의 값과 비교\n",
    "single_prediction = model(X_single_data)\n",
    "print(\"Prediction: \", torch.max(single_prediction.data, 1)[1])\n",
    "print(int(time()-t0)//60, 'min', int(time()-t0)%60, 'sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST 2\n",
    "Nural Network 를 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nn layers\n",
    "linear1 = torch.nn.Linear(784, 256, bias=True)\n",
    "linear2 = torch.nn.Linear(256, 256, bias=True)\n",
    "linear3 = torch.nn.Linear(256, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "# model\n",
    "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3)\n",
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1] cost = 0.301067621\n",
      "[Epoch:    2] cost = 0.11021772\n",
      "[Epoch:    3] cost = 0.0715760663\n",
      "[Epoch:    4] cost = 0.0528145395\n",
      "[Epoch:    5] cost = 0.0378407687\n",
      "[Epoch:    6] cost = 0.0306602456\n",
      "[Epoch:    7] cost = 0.0241072178\n",
      "[Epoch:    8] cost = 0.0203977656\n",
      "[Epoch:    9] cost = 0.0149436677\n",
      "[Epoch:   10] cost = 0.0167632103\n",
      "[Epoch:   11] cost = 0.0102889761\n",
      "[Epoch:   12] cost = 0.0124249188\n",
      "[Epoch:   13] cost = 0.0116662057\n",
      "[Epoch:   14] cost = 0.0102858394\n",
      "[Epoch:   15] cost = 0.0112185385\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "# 학습모델 만들기\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(mnist_train) // batch_size\n",
    "\n",
    "    for i, (batch_xs, batch_ys) in enumerate(data_loader):\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        X = Variable(batch_xs.view(-1, 28 * 28))\n",
    "        Y = Variable(batch_ys)    # label is not one-hot encoded\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost / total_batch\n",
    "    print(\"[Epoch: {:>4}] cost = {:>.9}\".format(epoch + 1, avg_cost.data[0]))\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9786\n"
     ]
    }
   ],
   "source": [
    "# 학습모델 테스트 및 정확도 측정\n",
    "X_test = Variable(mnist_test.test_data.view(-1, 28 * 28).float())\n",
    "Y_test = Variable(mnist_test.test_labels)\n",
    "prediction = model(X_test)\n",
    "correct_prediction = (torch.max(prediction.data, 1)[1] == Y_test.data)\n",
    "accuracy = correct_prediction.float().mean()\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  \n",
      " 0\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "Prediction:  \n",
      " 0\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "5 min 7 sec\n"
     ]
    }
   ],
   "source": [
    "# 임의 데이터 시뮬레이션\n",
    "r = random.randint(0, len(mnist_test) - 1)\n",
    "X_single_data = Variable(mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float())\n",
    "Y_single_data = Variable(mnist_test.test_labels[r:r + 1])\n",
    "print(\"Label: \", Y_single_data.data)\n",
    "single_prediction = model(X_single_data)\n",
    "print(\"Prediction: \", torch.max(single_prediction.data, 1)[1])\n",
    "print(int(time()-t0)//60, 'min', int(time()-t0)%60, 'sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MNIST 3\n",
    "and Xavier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    홍콩 중문대 박사과정에 있던 he가 2015년에 이 방식을 사용해서 ImageNet에서 3% 에러를 달성했다.\n",
    "    xavier - 입력값과 출력값 사이의 난수를 선택해서 입력값의 제곱근으로 나눈다.\n",
    "    he - 입력값을 반으로 나눈 제곱근 사용. 분모가 작아지기 때문에 xavier보다 넓은 범위의 난수 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.init\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xavier initializer\n",
    "torch.nn.init.xavier_uniform(linear1.weight)\n",
    "torch.nn.init.xavier_uniform(linear2.weight)\n",
    "torch.nn.init.xavier_uniform(linear3.weight)\n",
    "# model\n",
    "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3)\n",
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1] cost = 0.245956078\n",
      "[Epoch:    2] cost = 0.0926144123\n",
      "[Epoch:    3] cost = 0.0621828288\n",
      "[Epoch:    4] cost = 0.0444740318\n",
      "[Epoch:    5] cost = 0.0336061083\n",
      "[Epoch:    6] cost = 0.0265867803\n",
      "[Epoch:    7] cost = 0.0188468993\n",
      "[Epoch:    8] cost = 0.0207099058\n",
      "[Epoch:    9] cost = 0.0147647941\n",
      "[Epoch:   10] cost = 0.0157889128\n",
      "[Epoch:   11] cost = 0.0123811411\n",
      "[Epoch:   12] cost = 0.0109416591\n",
      "[Epoch:   13] cost = 0.00986913685\n",
      "[Epoch:   14] cost = 0.0111952536\n",
      "[Epoch:   15] cost = 0.0102181891\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "# 학습모델 만들기\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(mnist_train) // batch_size\n",
    "\n",
    "    for i, (batch_xs, batch_ys) in enumerate(data_loader):\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        X = Variable(batch_xs.view(-1, 28 * 28))\n",
    "        Y = Variable(batch_ys)    # label is not one-hot encoded\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost / total_batch\n",
    "    print(\"[Epoch: {:>4}] cost = {:>.9}\".format(epoch + 1, avg_cost.data[0]))\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9805\n"
     ]
    }
   ],
   "source": [
    "# 학습모델 테스트 및 정확도 측정\n",
    "X_test = Variable(mnist_test.test_data.view(-1, 28 * 28).float())\n",
    "Y_test = Variable(mnist_test.test_labels)\n",
    "\n",
    "prediction = model(X_test)\n",
    "correct_prediction = (torch.max(prediction.data, 1)[1] == Y_test.data)\n",
    "accuracy = correct_prediction.float().mean()\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  \n",
      " 4\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "Prediction:  \n",
      " 2\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "4 min 54 sec\n"
     ]
    }
   ],
   "source": [
    "# 임의 데이터 시뮬레이션\n",
    "r = random.randint(0, len(mnist_test) - 1)\n",
    "X_single_data = Variable(mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float())\n",
    "Y_single_data = Variable(mnist_test.test_labels[r:r + 1])\n",
    "\n",
    "print(\"Label: \", Y_single_data.data)\n",
    "single_prediction = model(X_single_data)\n",
    "print(\"Prediction: \", torch.max(single_prediction.data, 1)[1])\n",
    "print(int(time()-t0)//60, 'min', int(time()-t0)%60, 'sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MNIST 4\n",
    "and Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nn layers\n",
    "linear1 = torch.nn.Linear(784, 512, bias=True)\n",
    "linear2 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear3 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear4 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear5 = torch.nn.Linear(512, 10, bias=True)\n",
    "relu = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       " 0.0832  0.0549  0.0611  ...  -0.0484  0.0227 -0.0112\n",
       "-0.0071 -0.0764  0.0878  ...  -0.0620  0.0192 -0.0361\n",
       " 0.0199 -0.0953  0.0343  ...   0.0230 -0.0630 -0.0081\n",
       "          ...             ⋱             ...          \n",
       "-0.0297  0.0418 -0.0346  ...   0.0208  0.1000 -0.0837\n",
       " 0.0246  0.0823 -0.0464  ...  -0.0421 -0.0644  0.0678\n",
       "-0.0124 -0.0222  0.0353  ...  -0.0845 -0.0053  0.0457\n",
       "[torch.FloatTensor of size 10x512]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xavier initializer\n",
    "torch.nn.init.xavier_uniform(linear1.weight)\n",
    "torch.nn.init.xavier_uniform(linear2.weight)\n",
    "torch.nn.init.xavier_uniform(linear3.weight)\n",
    "torch.nn.init.xavier_uniform(linear4.weight)\n",
    "torch.nn.init.xavier_uniform(linear5.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model = torch.nn.Sequential(linear1, relu,\n",
    "                            linear2, relu,\n",
    "                            linear3, relu,\n",
    "                            linear4, relu,\n",
    "                            linear5)\n",
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1] cost = 0.210798308\n",
      "[Epoch:    2] cost = 0.0904840976\n",
      "[Epoch:    3] cost = 0.0658415332\n",
      "[Epoch:    4] cost = 0.0507572927\n",
      "[Epoch:    5] cost = 0.0417291224\n",
      "[Epoch:    6] cost = 0.032476861\n",
      "[Epoch:    7] cost = 0.0308252443\n",
      "[Epoch:    8] cost = 0.0255429968\n",
      "[Epoch:    9] cost = 0.0240809005\n",
      "[Epoch:   10] cost = 0.0192096625\n",
      "[Epoch:   11] cost = 0.0210121498\n",
      "[Epoch:   12] cost = 0.0213301592\n",
      "[Epoch:   13] cost = 0.0156960711\n",
      "[Epoch:   14] cost = 0.0140191251\n",
      "[Epoch:   15] cost = 0.0152498316\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "# 학습모델 만들기\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(mnist_train) // batch_size\n",
    "\n",
    "    for i, (batch_xs, batch_ys) in enumerate(data_loader):\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        X = Variable(batch_xs.view(-1, 28 * 28))\n",
    "        Y = Variable(batch_ys)    # label is not one-hot encoded\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost / total_batch\n",
    "    print(\"[Epoch: {:>4}] cost = {:>.9}\".format(epoch + 1, avg_cost.data[0]))\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9795\n"
     ]
    }
   ],
   "source": [
    "# 학습모델 테스트 및 정확도 측정\n",
    "X_test = Variable(mnist_test.test_data.view(-1, 28 * 28).float())\n",
    "Y_test = Variable(mnist_test.test_labels)\n",
    "\n",
    "prediction = model(X_test)\n",
    "correct_prediction = (torch.max(prediction.data, 1)[1] == Y_test.data)\n",
    "accuracy = correct_prediction.float().mean()\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  \n",
      " 1\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "Prediction:  \n",
      " 1\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "17 min 30 sec\n"
     ]
    }
   ],
   "source": [
    "# 임의 데이터 시뮬레이션\n",
    "r = random.randint(0, len(mnist_test) - 1)\n",
    "X_single_data = Variable(mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float())\n",
    "Y_single_data = Variable(mnist_test.test_labels[r:r + 1])\n",
    "\n",
    "print(\"Label: \", Y_single_data.data)\n",
    "single_prediction = model(X_single_data)\n",
    "print(\"Prediction: \", torch.max(single_prediction.data, 1)[1])\n",
    "print(int(time()-t0)//60, 'min', int(time()-t0)%60, 'sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MNIST 5\n",
    "and Deep Learning with Drop Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "keep_prob = 0.7\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nn layers\n",
    "linear1 = torch.nn.Linear(784, 512, bias=True)\n",
    "linear2 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear3 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear4 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear5 = torch.nn.Linear(512, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "# p is the probability of being dropped in PyTorch ( 전체의 70% 만큼만 활용 )\n",
    "dropout = torch.nn.Dropout(p=1 - keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       " 5.3671e-02 -9.6433e-02 -8.8157e-02  ...   2.5232e-02 -3.6547e-02 -3.3956e-03\n",
       "-2.3887e-02 -1.0675e-01  5.1505e-02  ...  -2.3071e-02  5.4573e-02 -5.9830e-02\n",
       "-6.4813e-02  4.7453e-02  2.1834e-02  ...   9.0064e-02 -7.8786e-02  1.0311e-01\n",
       "                ...                   ⋱                   ...                \n",
       "-2.8091e-02  9.7533e-02 -1.0102e-02  ...   3.2850e-02  5.1721e-02  8.7104e-02\n",
       "-4.4692e-02  6.7316e-02 -7.3007e-02  ...   9.0272e-02 -1.7859e-02 -6.6067e-02\n",
       " 7.0924e-02 -2.7705e-02 -8.6818e-02  ...   2.1309e-02  3.6259e-02 -5.2669e-02\n",
       "[torch.FloatTensor of size 10x512]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xavier initializer\n",
    "torch.nn.init.xavier_uniform(linear1.weight)\n",
    "torch.nn.init.xavier_uniform(linear2.weight)\n",
    "torch.nn.init.xavier_uniform(linear3.weight)\n",
    "torch.nn.init.xavier_uniform(linear4.weight)\n",
    "torch.nn.init.xavier_uniform(linear5.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 학습모델 정의하기\n",
    "model = torch.nn.Sequential(linear1, relu, dropout,\n",
    "                            linear2, relu, dropout,\n",
    "                            linear3, relu, dropout,\n",
    "                            linear4, relu, dropout,\n",
    "                            linear5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1] cost = 0.314977467\n",
      "[Epoch:    2] cost = 0.145359248\n",
      "[Epoch:    3] cost = 0.112980329\n",
      "[Epoch:    4] cost = 0.0965414718\n",
      "[Epoch:    5] cost = 0.082470648\n",
      "[Epoch:    6] cost = 0.0748037025\n",
      "[Epoch:    7] cost = 0.0692059323\n",
      "[Epoch:    8] cost = 0.0625941902\n",
      "[Epoch:    9] cost = 0.0597230643\n",
      "[Epoch:   10] cost = 0.0563570373\n",
      "[Epoch:   11] cost = 0.0526821129\n",
      "[Epoch:   12] cost = 0.0504432209\n",
      "[Epoch:   13] cost = 0.0460488014\n",
      "[Epoch:   14] cost = 0.0466552302\n",
      "[Epoch:   15] cost = 0.0416866988\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "# 학습모델 만들기\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(mnist_train) // batch_size\n",
    "\n",
    "    for i, (batch_xs, batch_ys) in enumerate(data_loader):\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        X = Variable(batch_xs.view(-1, 28 * 28))\n",
    "        Y = Variable(batch_ys)    # label is not one-hot encoded\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost / total_batch\n",
    "    print(\"[Epoch: {:>4}] cost = {:>.9}\".format(epoch + 1, avg_cost.data[0]))\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9812\n"
     ]
    }
   ],
   "source": [
    "# 학습모델 테스트 및 정확도 측정\n",
    "model.eval()    # 측정모드로 전환 (dropout = False)\n",
    "X_test = Variable(mnist_test.test_data.view(-1, 28 * 28).float())\n",
    "Y_test = Variable(mnist_test.test_labels)\n",
    "\n",
    "prediction = model(X_test)\n",
    "correct_prediction = (torch.max(prediction.data, 1)[1] == Y_test.data)\n",
    "accuracy = correct_prediction.float().mean()\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  \n",
      " 5\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "Prediction:  \n",
      " 5\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "17 min 25 sec\n"
     ]
    }
   ],
   "source": [
    "# 임의 데이터 시뮬레이션\n",
    "r = random.randint(0, len(mnist_test) - 1)\n",
    "X_single_data = Variable(mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float())\n",
    "Y_single_data = Variable(mnist_test.test_labels[r:r + 1])\n",
    "\n",
    "print(\"Label: \", Y_single_data.data)\n",
    "single_prediction = model(X_single_data)\n",
    "print(\"Prediction: \", torch.max(single_prediction.data, 1)[1])\n",
    "print(int(time()-t0)//60, 'min', int(time()-t0)%60, 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
