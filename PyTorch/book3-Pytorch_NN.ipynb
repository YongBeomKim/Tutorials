{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제3절 Nural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Warm up : Numpy\n",
    "Numpy를 활용한 2 layer network 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N : 배치 사이즈\n",
    "# H : hidden layer\n",
    "# D_in : 입력 dim\n",
    "# D_out : 출력 dim\n",
    "N, H, D_in , D_out = 64, 100, 1000, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 랜덤 데이터로 생성\n",
    "import numpy as np\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 랜덤한 weight 초기값 지정\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0  loss 24655411.825640  w1[0][0]: -1.431834 w1[0][0]: 1.317450\n",
      "  250  loss 0.124313  w1[0][0]: -1.407407 w1[0][0]: 0.961950\n",
      "  500  loss 0.000000  w1[0][0]: -1.407407 w1[0][0]: 0.962001\n",
      "  750  loss 0.000000  w1[0][0]: -1.407407 w1[0][0]: 0.962001\n",
      " 1000  loss 0.000000  w1[0][0]: -1.407407 w1[0][0]: 0.962001\n",
      " 1250  loss 0.000000  w1[0][0]: -1.407407 w1[0][0]: 0.962001\n",
      " 1500  loss 0.000000  w1[0][0]: -1.407407 w1[0][0]: 0.962001\n",
      " 1750  loss 0.000000  w1[0][0]: -1.407407 w1[0][0]: 0.962001\n",
      " 2000  loss 0.000000  w1[0][0]: -1.407407 w1[0][0]: 0.962001\n",
      " 2250  loss 0.000000  w1[0][0]: -1.407407 w1[0][0]: 0.962001\n",
      " 2500  loss 0.000000  w1[0][0]: -1.407407 w1[0][0]: 0.962001\n",
      " 2750  loss 0.000000  w1[0][0]: -1.407407 w1[0][0]: 0.962001\n",
      " 3000  loss 0.000000  w1[0][0]: -1.407407 w1[0][0]: 0.962001\n",
      " 3250  loss 0.000000  w1[0][0]: -1.407407 w1[0][0]: 0.962001\n",
      " 3500  loss 0.000000  w1[0][0]: -1.407407 w1[0][0]: 0.962001\n",
      " 3750  loss 0.000000  w1[0][0]: -1.407407 w1[0][0]: 0.962001\n",
      " 4000  loss 0.000000  w1[0][0]: -1.407407 w1[0][0]: 0.962001\n",
      " 4250  loss 0.000000  w1[0][0]: -1.407407 w1[0][0]: 0.962001\n",
      " 4500  loss 0.000000  w1[0][0]: -1.407407 w1[0][0]: 0.962001\n",
      " 4750  loss 0.000000  w1[0][0]: -1.407407 w1[0][0]: 0.962001\n",
      " 5000  loss 0.000000  w1[0][0]: -1.407407 w1[0][0]: 0.962001\n",
      "11 sec\n"
     ]
    }
   ],
   "source": [
    "# 학습모델 생성하기\n",
    "t0 = time()\n",
    "for t in range(5001):\n",
    "    # Forward 모형 만들기 \n",
    "    h = x.dot(w1)  \n",
    "    h_relu = np.maximum(h,0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    # Loss 함수 계산\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    #print(t,loss)\n",
    "\n",
    "    # w1, w2로 계산된 모델의 Backprop 를 계산\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weight (회귀식의 weight 값 바꾸기)\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    \n",
    "    if t % 250 == 0:\n",
    "        print(\" {0:4}  loss {1:6f}  w1[0][0]: {2:8f} w1[0][0]: {3:8f}\".format(\n",
    "                t,loss, w1[0][0], w2[0][0]))\n",
    "print(int(time()-t0), 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.4074073965209111"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  2],\n",
       "       [ 6, 15]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy.dot()  # 배열 곱 (내적)\n",
    "import numpy as np\n",
    "a = [ [2,0], [0,3] ]\n",
    "b = [ [4,1], [2,5] ]\n",
    "np.dot(a,b)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 5, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy.maximum()  # 최대 우도값을 갖는 배열의 값\n",
    "np.maximum([2,3,4], [1,5,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Pytorch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "dtype = torch.FloatTensor\n",
    "# dtype = torch.cuda.FloatTensor # GPU 활용을 위한 객체정의\n",
    "N, H, D_in , D_out = 64, 100, 1000, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 랜덤함수로 nput, output data 생성하기\n",
    "x = torch.randn(N, D_in).type(dtype)\n",
    "y = torch.randn(N, D_out).type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 학습모델의 초기값을 난수함수를 활용\n",
    "w1 = torch.randn(D_in, H).type(dtype)\n",
    "w2 = torch.randn(H, D_out).type(dtype)\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0  loss 25133330.768107  w1[0][0]: -0.421055 w1[0][0]: 1.084256\n",
      "  250  loss 0.139304  w1[0][0]: -0.444530 w1[0][0]: 0.455891\n",
      "  500  loss 0.000053  w1[0][0]: -0.444532 w1[0][0]: 0.455908\n",
      "  750  loss 0.000009  w1[0][0]: -0.444532 w1[0][0]: 0.455909\n",
      " 1000  loss 0.000004  w1[0][0]: -0.444532 w1[0][0]: 0.455909\n",
      " 1250  loss 0.000003  w1[0][0]: -0.444532 w1[0][0]: 0.455910\n",
      " 1500  loss 0.000002  w1[0][0]: -0.444532 w1[0][0]: 0.455910\n",
      " 1750  loss 0.000002  w1[0][0]: -0.444532 w1[0][0]: 0.455910\n",
      " 2000  loss 0.000001  w1[0][0]: -0.444532 w1[0][0]: 0.455910\n",
      " 2250  loss 0.000001  w1[0][0]: -0.444532 w1[0][0]: 0.455910\n",
      " 2500  loss 0.000001  w1[0][0]: -0.444532 w1[0][0]: 0.455910\n",
      " 2750  loss 0.000001  w1[0][0]: -0.444532 w1[0][0]: 0.455910\n",
      " 3000  loss 0.000001  w1[0][0]: -0.444532 w1[0][0]: 0.455910\n",
      " 3250  loss 0.000001  w1[0][0]: -0.444532 w1[0][0]: 0.455910\n",
      " 3500  loss 0.000000  w1[0][0]: -0.444532 w1[0][0]: 0.455910\n",
      " 3750  loss 0.000000  w1[0][0]: -0.444532 w1[0][0]: 0.455910\n",
      " 4000  loss 0.000000  w1[0][0]: -0.444532 w1[0][0]: 0.455910\n",
      " 4250  loss 0.000000  w1[0][0]: -0.444532 w1[0][0]: 0.455910\n",
      " 4500  loss 0.000000  w1[0][0]: -0.444532 w1[0][0]: 0.455910\n",
      " 4750  loss 0.000000  w1[0][0]: -0.444532 w1[0][0]: 0.455910\n",
      " 5000  loss 0.000000  w1[0][0]: -0.444532 w1[0][0]: 0.455910\n",
      "5 sec\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "# 학습모델 생성하기\n",
    "for t in range(5001):\n",
    "    # Forward 모델 실행\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    # loss 함수 실행\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    #print(t, loss)\n",
    "    \n",
    "    # backprop 계산\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[ h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # w1, w2 값을 바꿔가며 학습\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2    \n",
    "    \n",
    "    if t % 250 == 0:\n",
    "        print(\" {0:4}  loss {1:6f}  w1[0][0]: {2:8f} w1[0][0]: {3:8f}\".format(\n",
    "                t,loss, w1[0][0], w2[0][0]))\n",
    "print(int(time()-t0), 'sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Pytorch Tensor - GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# dtype = torch.FloatTensor\n",
    "dtype = torch.cuda.FloatTensor # GPU 활용을 위한 객체정의\n",
    "N, H, D_in , D_out = 64, 100, 1000, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 랜덤함수로 nput, output data 생성하기\n",
    "x = torch.randn(N, D_in).type(dtype)\n",
    "y = torch.randn(N, D_out).type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습모델의 초기값을 난수함수를 활용\n",
    "w1 = torch.randn(D_in, H).type(dtype)\n",
    "w2 = torch.randn(H, D_out).type(dtype)\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0  loss 35619076.000000  w1[0][0]: -0.702226 w1[0][0]: -1.751028\n",
      "  250  loss 0.034436  w1[0][0]: -0.657332 w1[0][0]: -1.514559\n",
      "  500  loss 0.000021  w1[0][0]: -0.657334 w1[0][0]: -1.514532\n",
      "  750  loss 0.000005  w1[0][0]: -0.657334 w1[0][0]: -1.514532\n",
      " 1000  loss 0.000003  w1[0][0]: -0.657334 w1[0][0]: -1.514532\n",
      " 1250  loss 0.000002  w1[0][0]: -0.657334 w1[0][0]: -1.514532\n",
      " 1500  loss 0.000001  w1[0][0]: -0.657334 w1[0][0]: -1.514532\n",
      " 1750  loss 0.000001  w1[0][0]: -0.657334 w1[0][0]: -1.514532\n",
      " 2000  loss 0.000001  w1[0][0]: -0.657334 w1[0][0]: -1.514532\n",
      " 2250  loss 0.000001  w1[0][0]: -0.657334 w1[0][0]: -1.514532\n",
      " 2500  loss 0.000001  w1[0][0]: -0.657334 w1[0][0]: -1.514532\n",
      " 2750  loss 0.000000  w1[0][0]: -0.657334 w1[0][0]: -1.514532\n",
      " 3000  loss 0.000000  w1[0][0]: -0.657334 w1[0][0]: -1.514532\n",
      " 3250  loss 0.000000  w1[0][0]: -0.657334 w1[0][0]: -1.514532\n",
      " 3500  loss 0.000000  w1[0][0]: -0.657334 w1[0][0]: -1.514532\n",
      " 3750  loss 0.000000  w1[0][0]: -0.657334 w1[0][0]: -1.514532\n",
      " 4000  loss 0.000000  w1[0][0]: -0.657334 w1[0][0]: -1.514532\n",
      " 4250  loss 0.000000  w1[0][0]: -0.657334 w1[0][0]: -1.514532\n",
      " 4500  loss 0.000000  w1[0][0]: -0.657334 w1[0][0]: -1.514532\n",
      " 4750  loss 0.000000  w1[0][0]: -0.657334 w1[0][0]: -1.514532\n",
      " 5000  loss 0.000000  w1[0][0]: -0.657334 w1[0][0]: -1.514532\n",
      "2 sec\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "# 학습모델 생성하기\n",
    "for t in range(5001):\n",
    "    # Forward 모델 실행\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    # loss 함수 실행\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    #print(t, loss)\n",
    "    \n",
    "    # backprop 계산\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[ h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # w1, w2 값을 바꿔가며 학습\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2    \n",
    "    \n",
    "    if t % 250 == 0:\n",
    "        print(\" {0:4}  loss {1:6f}  w1[0][0]: {2:8f} w1[0][0]: {3:8f}\".format(\n",
    "                t,loss, w1[0][0], w2[0][0]))\n",
    "print(int(time()-t0), 'sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 Auto Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Variables 와 autograd\n",
    "<p>autograd 패키지 사용시 forward pass 는 계산그래프로 저장</p></br>\n",
    "<p>Node 는 Tensor,  edge 는 입/출력 Tensor를 생성하는 함수가 된다.</p></br>\n",
    "<p>Pytorch의 Variable은 Pytorch의 Tensor와 동일 API를 갖는다. </p></br>\n",
    "<p>단 차이는 Variable을 사용하면 계산 그래프를 정한다.</p></br>\n",
    "<p>위의 2 layer의 Nural Network를 작성해보자 (backward pass 를 따로 매뉴얼로 처리할 필요가 없다)</p></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "# dtype = torch.FloatTensor\n",
    "dtype = torch.cuda.FloatTensor # GPU 활용을 위한 객체정의\n",
    "N, H, D_in , D_out = 64, 100, 1000, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
    "\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True) \n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0  loss 32834778.000000  \n",
      "   25  loss 149842.562500  \n",
      "   50  loss 17918.035156  \n",
      "   75  loss 3263.335938  \n",
      "  100  loss 705.739319  \n",
      "  125  loss 169.641891  \n",
      "  150  loss 43.786037  \n",
      "  175  loss 11.928554  \n",
      "  200  loss 3.377903  \n",
      "  225  loss 0.984798  \n",
      "  250  loss 0.293466  \n",
      "  275  loss 0.089065  \n",
      "  300  loss 0.027453  \n",
      "  325  loss 0.008713  \n",
      "  350  loss 0.002943  \n",
      "  375  loss 0.001119  \n",
      "  400  loss 0.000490  \n",
      "  425  loss 0.000247  \n",
      "  450  loss 0.000139  \n",
      "  475  loss 0.000086  \n",
      "  500  loss 0.000058  \n"
     ]
    }
   ],
   "source": [
    "for t in range(501):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    loss.backward() # backward pass를 계산하는 함수 - 1줄로 처리 끝!!\n",
    "    \n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "    \n",
    "    w1.grad.data.zero_() #weight 갱신 후, gradient를 0으로 초기화\n",
    "    w2.grad.data.zero_()\n",
    "    \n",
    "    if t % 25 == 0:\n",
    "        print(\" {0:4}  loss {1:8f}  \".format(\n",
    "                t,loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 nn (module)\n",
    "http://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_nn.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Pytorch : nn\n",
    "<p>대규모 Nural Network에서는 raw_autograd 값이 너무 작은 값을 출력하게 된다.</p></br>\n",
    "<p>개별 학습시 다양한 고도화 모듈을 쉽게 적용하기 위한 Tensorflow-slim, TFLearn등이 있다</p></br>\n",
    "<p>Pytorch의 'nn 패키지'는 모듈의 세트를 정의하는데, Nural Network의 Layer와 유사하다</p></br>\n",
    "<p>nn 모듈은 '입력 Variable' 과 '출력 Variable'을 계산하는 'internal state를 갖는다</p></br>\n",
    "<p>nn 모듈에서 loss function은 세트로 정의를 한다</p></br>\n",
    "<p>위의 2 layer의 Nural Network를 'nn 패키지'로 작성해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    A fully-connected ReLU network with one hidden layer, \n",
    "    trained to predict y from x by minimizing squared Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#dtype = torch.cuda.FloatTensor # GPU 활용을 위한 객체정의\n",
    "N, H, D_in , D_out = 64, 100, 1000, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "# x = Variable(torch.randn(N, D_in).type(dtype))\n",
    "# y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
    "\n",
    "# 평균제곱오차 (mean squared error between n elements) 로 loss 함수를 정의한다\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)  # loss_fn = torch.nn.MSELoss(size_average=False).type(dtype)\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True) \n",
    "# w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n",
    "\n",
    "# torch.nn.Sequential()은 내부에 정의된 모듈들을 시퀀스로 적용한다\n",
    "# 각각의 선형 모듈은 선형함수로 입출력을 계산하며, 내부 Variable 에 weight와 bias를 값을 저장한다\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),     # 입력 Edge\n",
    "          torch.nn.ReLU(),              # Hidden Edge\n",
    "          torch.nn.Linear(H, D_out),)   # 출력 Edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0  loss 644.098145  \n",
      "   25  loss 150.609634  \n",
      "   50  loss 30.325203  \n",
      "   75  loss 7.045473  \n",
      "  100  loss 2.007695  \n",
      "  125  loss 0.682817  \n",
      "  150  loss 0.266137  \n",
      "  175  loss 0.113361  \n",
      "  200  loss 0.051243  \n",
      "  225  loss 0.024188  \n",
      "  250  loss 0.011997  \n",
      "  275  loss 0.006231  \n",
      "  300  loss 0.003360  \n",
      "  325  loss 0.001870  \n",
      "  350  loss 0.001069  \n",
      "  375  loss 0.000624  \n",
      "  400  loss 0.000371  \n",
      "  425  loss 0.000223  \n",
      "  450  loss 0.000136  \n",
      "  475  loss 0.000083  \n",
      "  500  loss 0.000051  \n"
     ]
    }
   ],
   "source": [
    "for t in range(501):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    model.zero_grad() # backward pass를 계산하기 전에 gradient를 초기화 한다 \n",
    "    loss.backward()   # y 의 requires_grad=True를 설정하면 개별 파라미터의 계산 과정들이 저장된다 (용량과 메모리 압박이 문제)\n",
    "    \n",
    "    if t % 25 == 0:\n",
    "        print(\" {0:4}  loss {1:8f}  \".format(\n",
    "                t,loss.data[0]))\n",
    "    for param in model.parameters():\n",
    "        param.data -= learning_rate * param.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Pytorch : optim\n",
    "<p>지금까지 파라미터의 Variable의 .data 맴버들을 수작업으로 계산을 하며 weight를 업데이트 했다</p></br>\n",
    "<p>단순 모델이 아닌 AdaGrad, PMSProp, Adam과 같은 복잡한 optimizer에는 비효율적이다.</p></br>\n",
    "<p>Pytorch의 'optim 패키지'를 통해서 Adam 알고리즘을 이용하여 모델을 최적화 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "N, H, D_in , D_out = 64, 100, 1000, 10\n",
    "\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "learning_rate = 1e-4\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),     # 입력 Edge\n",
    "          torch.nn.ReLU(),              # Hidden Edge\n",
    "          torch.nn.Linear(H, D_out),)   # 출력 Edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optim 패키지에는 여러 최적화 알고리즘이 포함되어 있다 (이를 통해서 weight를 업데이트 한다)\n",
    "# Adam 알고리즘을 사용하고, 첫 argument는 Variable의 최초 작동을 명령한다.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0  loss 735.333313  \n",
      "   25  loss 412.624908  \n",
      "   50  loss 243.404099  \n",
      "   75  loss 136.422836  \n",
      "  100  loss 68.768234  \n",
      "  125  loss 30.495417  \n",
      "  150  loss 11.748485  \n",
      "  175  loss  4.069880  \n",
      "  200  loss  1.327747  \n",
      "  225  loss  0.428611  \n",
      "  250  loss  0.139654  \n",
      "  275  loss  0.045835  \n",
      "  300  loss  0.015115  \n",
      "  325  loss  0.004961  \n",
      "  350  loss  0.001596  \n",
      "  375  loss  0.000499  \n",
      "  400  loss  0.000151  \n",
      "  425  loss  0.000044  \n",
      "  450  loss  0.000012  \n",
      "  475  loss  0.000003  \n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    # backward pass를 계산하기 전 위에서는 모델 전체를 초기화 했지만 cf) model.zero_grad(),\n",
    "    # 여기는 optimizer 만 초기화 한다\n",
    "    optimizer.zero_grad()  \n",
    "    loss.backward()\n",
    "    optimizer.step()  # 파라미터를 업데이트 하는 optimizer 의 step function을 호출한다\n",
    "    if t % 25 == 0:\n",
    "        print(\" {0:4}  loss {1:9f}  \".format(\n",
    "                t,loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
